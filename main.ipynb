{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13312751,"sourceType":"datasetVersion","datasetId":8435984},{"sourceId":13552943,"sourceType":"datasetVersion","datasetId":8607893}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F\n\nclass RatingDataset(Dataset):\n    def __init__(self, users, movies, ratings):\n        self.users = torch.LongTensor(users)\n        self.movies = torch.LongTensor(movies)\n        self.ratings = torch.FloatTensor(ratings)\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        return self.users[idx], self.movies[idx], self.ratings[idx]\n\n\nclass GMF(nn.Module):\n    def __init__(self, n_users, n_movies, embedding_dim, global_mean=0.0):\n        super().__init__()\n\n        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)\n\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.movie_bias = nn.Embedding(n_movies, 1)\n\n        self.affine_output = nn.Linear(embedding_dim, 1, bias=False)\n        self.global_mean = global_mean\n\n        nn.init.normal_(self.user_embedding.weight, std=0.01)\n        nn.init.normal_(self.movie_embedding.weight, std=0.01)\n        nn.init.zeros_(self.user_bias.weight)\n        nn.init.zeros_(self.movie_bias.weight)\n        nn.init.xavier_uniform_(self.affine_output.weight)\n\n    def forward(self, user_ids, movie_ids):\n        u = self.user_embedding(user_ids)\n        m = self.movie_embedding(movie_ids)\n\n        interaction = self.affine_output(u * m).squeeze(-1)\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.movie_bias(movie_ids).squeeze(-1)\n\n        return interaction + bu + bi + self.global_mean\n        \nclass DMF(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, user_hidden_dims, item_hidden_dims, dropout, global_mean=0.0):\n        super().__init__()\n        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.item_bias = nn.Embedding(n_items, 1)\n        self.global_mean = global_mean\n\n        u_layers = []\n        in_dim = embedding_dim\n        for h in user_hidden_dims:\n            u_layers += [\n                nn.Linear(in_dim, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ]\n            in_dim = h\n        self.user_mlp = nn.Sequential(*u_layers)\n\n        i_layers = []\n        in_dim = embedding_dim\n        for h in item_hidden_dims:\n            i_layers += [\n                nn.Linear(in_dim, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ]\n            in_dim = h\n        self.item_mlp = nn.Sequential(*i_layers)\n        self.user_proj = nn.Linear(user_hidden_dims[-1], embedding_dim)\n        self.item_proj = nn.Linear(item_hidden_dims[-1], embedding_dim)\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.normal_(self.user_embedding.weight, std=0.01)\n        nn.init.normal_(self.item_embedding.weight, std=0.01)\n        nn.init.zeros_(self.user_bias.weight)\n        nn.init.zeros_(self.item_bias.weight)\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, user_ids, item_ids):\n        u = self.user_embedding(user_ids)\n        i = self.item_embedding(item_ids)\n\n        u = self.user_mlp(u)\n        i = self.item_mlp(i)\n\n        u = self.user_proj(u)\n        i = self.item_proj(i)\n\n        interaction = (u * i).sum(dim=-1)\n\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.item_bias(item_ids).squeeze(-1)\n\n        return interaction + bu + bi + self.global_mean\n        \n\nclass AttentionBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        if x.dim() == 2:\n            x = x.unsqueeze(1)\n        attn_out, _ = self.attn(x, x, x)\n        x = self.norm1(x + self.relu(self.dropout(attn_out)))\n        x = self.norm2(x + self.dropout(self.relu(self.fc(x))))\n        return x.squeeze(1)\n\n\nclass NeuMF(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim=64, mlp_hidden_dims=(128, 64), dropout=0.1, global_mean=0.0):\n        super().__init__()\n\n        self.user_embedding_gmf = nn.Embedding(n_users, embedding_dim)\n        self.item_embedding_gmf = nn.Embedding(n_items, embedding_dim)\n\n        self.user_embedding_mlp = nn.Embedding(n_users, embedding_dim)\n        self.item_embedding_mlp = nn.Embedding(n_items, embedding_dim)\n\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.item_bias = nn.Embedding(n_items, 1)\n        self.global_mean = global_mean\n\n        layers = []\n        input_dim = embedding_dim * 2\n        for h in mlp_hidden_dims:\n            layers += [\n                nn.Linear(input_dim, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ]\n            input_dim = h\n        self.mlp = nn.Sequential(*layers)\n\n        self.output_layer = nn.Linear(embedding_dim + input_dim, 1)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for emb in [\n            self.user_embedding_gmf,\n            self.item_embedding_gmf,\n            self.user_embedding_mlp,\n            self.item_embedding_mlp\n        ]:\n            nn.init.normal_(emb.weight, std=0.01)\n\n        nn.init.zeros_(self.user_bias.weight)\n        nn.init.zeros_(self.item_bias.weight)\n        nn.init.xavier_uniform_(self.output_layer.weight)\n\n    def forward(self, user_ids, item_ids):\n        u_gmf = self.user_embedding_gmf(user_ids)\n        i_gmf = self.item_embedding_gmf(item_ids)\n        gmf_out = u_gmf * i_gmf\n\n        u_mlp = self.user_embedding_mlp(user_ids)\n        i_mlp = self.item_embedding_mlp(item_ids)\n        mlp_input = torch.cat([u_mlp, i_mlp], dim=-1)\n        mlp_out = self.mlp(mlp_input)\n\n        fusion = torch.cat([gmf_out, mlp_out], dim=-1)\n        pred = self.output_layer(fusion).squeeze(-1)\n\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.item_bias(item_ids).squeeze(-1)\n\n        return pred + bu + bi + self.global_mean\n\n\nclass AttentionNet(nn.Module):\n    def __init__(self, n_users, n_movies, embedding_dim, hidden_dims,\n                 n_attention_blocks, dropout, num_heads=4, global_mean=0.0):\n        super().__init__()\n\n        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)\n\n        self.feature_dim = hidden_dims[-1]\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.movie_bias = nn.Embedding(n_movies, 1)\n\n        self.global_mean = global_mean\n\n        self.attention_blocks = nn.ModuleList([\n            AttentionBlock(embedding_dim * 2, num_heads, dropout)\n            for _ in range(n_attention_blocks)\n        ])\n\n        layers = []\n        dim = embedding_dim * 2\n        for h in hidden_dims:\n            layers += [\n                nn.Linear(dim, h),\n                nn.ReLU(),\n                nn.LayerNorm(h),\n                nn.Dropout(dropout)\n            ]\n            dim = h\n\n        self.mlp_features = nn.Sequential(*layers)\n        self.final_layer = nn.Linear(dim, 1)\n\n        nn.init.normal_(self.user_embedding.weight, std=0.01)\n        nn.init.normal_(self.movie_embedding.weight, std=0.01)\n        nn.init.zeros_(self.user_bias.weight)\n        nn.init.zeros_(self.movie_bias.weight)\n\n    def forward(self, user_ids, movie_ids, return_features=False):\n        u = self.user_embedding(user_ids)\n        m = self.movie_embedding(movie_ids)\n\n        x = torch.cat([u, m], dim=-1)\n        for block in self.attention_blocks:\n            x = block(x)\n\n        features = self.mlp_features(x)\n\n        if return_features:\n            return features\n\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.movie_bias(movie_ids).squeeze(-1)\n\n        out = self.final_layer(features).squeeze(-1)\n        return out + bu + bi + self.global_mean\n\n\nclass FusionNCF(nn.Module):\n    def __init__(self, gmf_model, attn_model, dropout=0.1, global_mean=0.0):\n        super().__init__()\n\n        self.gmf = gmf_model\n        self.attn_net = attn_model\n        self.global_mean = global_mean\n\n        self.user_bias = gmf_model.user_bias\n        self.movie_bias = gmf_model.movie_bias\n\n        gmf_dim = gmf_model.user_embedding.embedding_dim\n        attn_dim = attn_model.feature_dim\n\n        self.attn_layer = nn.MultiheadAttention(\n            embed_dim=gmf_dim + attn_dim,\n            num_heads=4,\n            batch_first=True\n        )\n\n        self.fusion_layer = nn.Linear(gmf_dim + attn_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        nn.init.xavier_uniform_(self.fusion_layer.weight)\n\n    def forward(self, user_ids, movie_ids):\n        u = self.gmf.user_embedding(user_ids)\n        m = self.gmf.movie_embedding(movie_ids)\n        gmf_vec = u * m\n\n        attn_vec = self.attn_net(user_ids, movie_ids, return_features=True)\n\n        x = torch.cat([gmf_vec, attn_vec], dim=-1).unsqueeze(1)\n        x, _ = self.attn_layer(x, x, x)\n        x = self.dropout(x.squeeze(1))\n\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.movie_bias(movie_ids).squeeze(-1)\n\n        out = self.fusion_layer(x).squeeze(-1)\n        return out + bu + bi + self.global_mean\n        \n\nclass LightGCN(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, n_layers, edge_index, global_mean=0.0, learn_global_mean=False):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.embedding_dim = embedding_dim\n        self.n_layers = n_layers\n\n        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n\n        nn.init.normal_(self.user_embedding.weight, std=0.01)\n        nn.init.normal_(self.item_embedding.weight, std=0.01)\n\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.item_bias = nn.Embedding(n_items, 1)\n\n        nn.init.zeros_(self.user_bias.weight)\n        nn.init.zeros_(self.item_bias.weight)\n\n        if learn_global_mean:\n            self.global_mean = nn.Parameter(torch.tensor(global_mean))\n        else:\n            self.register_buffer(\"global_mean\", torch.tensor(global_mean))\n\n        self.register_buffer(\"norm_adj\", self.build_norm_adj(edge_index))\n\n    def build_norm_adj(self, edge_index):\n        device = edge_index.device\n        num_nodes = self.n_users + self.n_items\n\n        u = edge_index[0]\n        i = edge_index[1] + self.n_users\n\n        row = torch.cat([u, i])\n        col = torch.cat([i, u])\n\n        values = torch.ones(row.size(0), device=device)\n\n        adj = torch.sparse_coo_tensor(\n            torch.stack([row, col]),\n            values,\n            (num_nodes, num_nodes)\n        ).coalesce()\n\n        deg = torch.sparse.sum(adj, dim=1).to_dense()\n        deg_inv_sqrt = torch.pow(deg + 1e-8, -0.5)\n        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n\n        r, c = adj.indices()\n        norm_values = deg_inv_sqrt[r] * adj.values() * deg_inv_sqrt[c]\n\n        norm_adj = torch.sparse_coo_tensor(\n            adj.indices(),\n            norm_values,\n            adj.size()\n        )\n\n        return norm_adj\n\n    def propagate(self):\n        all_embeddings = torch.cat(\n            [self.user_embedding.weight, self.item_embedding.weight], dim=0\n        )\n\n        embeddings = [all_embeddings]\n\n        for _ in range(self.n_layers):\n            all_embeddings = torch.sparse.mm(self.norm_adj, all_embeddings)\n            embeddings.append(all_embeddings)\n\n        final_embeddings = torch.mean(torch.stack(embeddings, dim=0), dim=0)\n        users, items = torch.split(\n            final_embeddings, [self.n_users, self.n_items]\n        )\n\n        return users, items\n        \n    def propagate_perturbed(self, eps=0.1):\n        all_embeddings = torch.cat(\n            [self.user_embedding.weight, self.item_embedding.weight], dim=0\n        )\n    \n        embeddings = [all_embeddings]\n    \n        for _ in range(self.n_layers):\n            all_embeddings = torch.sparse.mm(self.norm_adj, all_embeddings)\n            noise = F.normalize(torch.rand_like(all_embeddings), dim=-1)\n            all_embeddings = all_embeddings + eps * noise\n            embeddings.append(all_embeddings)\n    \n        final_embeddings = torch.mean(torch.stack(embeddings, dim=0), dim=0)\n        return torch.split(final_embeddings, [self.n_users, self.n_items])\n\n    def forward(self, user_ids, item_ids):\n        user_emb, item_emb = self.propagate()\n\n        u = user_emb[user_ids]\n        i = item_emb[item_ids]\n\n        interaction = (u * i).sum(dim=-1)\n\n        bu = self.user_bias(user_ids).squeeze(-1)\n        bi = self.item_bias(item_ids).squeeze(-1)\n\n        return interaction + bu + bi + self.global_mean\n\n\nclass LightGCNPP(LightGCN):\n    def __init__(self, *args, residual=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.residual = residual\n        self.layer_weights = nn.Parameter(\n            torch.ones(self.n_layers + 1)\n        )\n\n    def propagate(self):\n        all_embeddings = torch.cat(\n            [self.user_embedding.weight, self.item_embedding.weight], dim=0\n        )\n\n        embeddings = [all_embeddings]\n\n        for _ in range(self.n_layers):\n            neigh = torch.sparse.mm(self.norm_adj, all_embeddings)\n            all_embeddings = neigh + all_embeddings if self.residual else neigh\n            embeddings.append(all_embeddings)\n\n        stack = torch.stack(embeddings, dim=0)\n        alpha = torch.softmax(self.layer_weights, dim=0)\n        final_embeddings = torch.sum(alpha[:, None, None] * stack, dim=0)\n\n        return torch.split(final_embeddings, [self.n_users, self.n_items])\n\nclass SimGCL(LightGCN):\n    def __init__(self, *args, eps=0.1, temperature=0.2, lambda_cl=0.1, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.eps = eps\n        self.temperature = temperature\n        self.lambda_cl = lambda_cl\n    \n    def contrastive_loss(self, users, items):\n        u1, i1 = self.propagate_perturbed(self.eps)\n        u2, i2 = self.propagate_perturbed(self.eps)\n        \n        loss_u = info_nce_loss(u1[users], u2[users], self.temperature)\n        loss_i = info_nce_loss(i1[items], i2[items], self.temperature)\n        return loss_u + loss_i\n\n\ndef info_nce_loss(z1, z2, temperature=0.2):\n    z1 = F.normalize(z1, dim=1)\n    z2 = F.normalize(z2, dim=1)\n    \n    \n    pos = torch.exp(torch.sum(z1 * z2, dim=1) / temperature)\n    ttl = torch.exp(torch.matmul(z1, z2.t()) / temperature).sum(dim=1)\n    return -torch.log(pos / ttl).mean()\n\ndef build_edge_index(df):\n    users = torch.LongTensor(df['user_idx'].values)\n    items = torch.LongTensor(df['movie_idx'].values)\n    return torch.stack([users, items], dim=0)\n\nclass Ensemble(nn.Module):\n    def __init__(self, models, learn_weights=True):\n        super().__init__()\n\n        self.models = nn.ModuleList(models)\n\n        for m in self.models:\n            for p in m.parameters():\n                p.requires_grad = False\n\n        n_models = len(models)\n\n        if learn_weights:\n            self.weights = nn.Parameter(torch.ones(n_models) / n_models)\n        else:\n            self.register_buffer(\"weights\", torch.ones(n_models) / n_models)\n\n    def forward(self, users, items):\n        preds = []\n        for model in self.models:\n            preds.append(model(users, items))\n\n        preds = torch.stack(preds, dim=0)\n        weights = torch.softmax(self.weights, dim=0)\n\n        return (weights[:, None] * preds).sum(dim=0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:16:22.872309Z","iopub.execute_input":"2026-01-10T11:16:22.873118Z","iopub.status.idle":"2026-01-10T11:16:22.919423Z","shell.execute_reply.started":"2026-01-10T11:16:22.873087Z","shell.execute_reply":"2026-01-10T11:16:22.918697Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def preprocessing(data_path, test_size=0.02, random_state=42):\n    if not os.path.exists(data_path):\n        print(\"Data file not found. Generating dummy data...\")\n        users = np.random.randint(0, 100, 10000)\n        movies = np.random.randint(0, 200, 10000)\n        ratings = np.random.randint(1, 6, 10000)\n        df = pd.DataFrame({'userId': users, 'movieId': movies, 'rating': ratings})\n    else:\n        df = pd.read_csv(data_path, sep='\\t', names=['userId', 'movieId', 'rating'])\n\n    user_ids = df['userId'].unique()\n    movie_ids = df['movieId'].unique()\n\n    min_rating = df['rating'].min()\n    max_rating = df['rating'].max()\n    print(f\"Scaling ratings from [{min_rating}, {max_rating}] to [0, 1].\")\n    df['rating'] = (df['rating'] - min_rating) / (max_rating - min_rating)\n\n    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n\n    df['user_idx'] = df['userId'].map(user_map)\n    df['movie_idx'] = df['movieId'].map(movie_map)\n\n    n_users = len(user_map)\n    n_movies = len(movie_map)\n\n    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n    val_size_relative = test_size / (1 - test_size)\n    train_df, val_df = train_test_split(train_val_df, test_size=val_size_relative, random_state=random_state)\n\n    return train_df, val_df, test_df, user_map, movie_map, n_users, n_movies, min_rating, max_rating\n\n\ndef train(model, train_df, val_df, batch_size, epochs, learning_rate, weight_decay, name, device, min_rating, max_rating, lambda_cl=0.1, temperature=0.2):\n    print(f\"\\nTraining Model: {name}\")\n\n    train_dataset = RatingDataset(\n        train_df['user_idx'].values,\n        train_df['movie_idx'].values,\n        train_df['rating'].values\n    )\n    val_dataset = RatingDataset(\n        val_df['user_idx'].values,\n        val_df['movie_idx'].values,\n        val_df['rating'].values\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    mse_criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2\n    )\n\n    best_val_loss = float('inf')\n    scaling_factor = max_rating - min_rating\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n\n        for users, pos_items, ratings in train_loader:\n            users = users.to(device)\n            pos_items = pos_items.to(device)\n            ratings = ratings.to(device)\n\n            optimizer.zero_grad()\n\n            predictions = model(users, pos_items)\n            rating_loss = mse_criterion(predictions, ratings)\n\n            cl_loss = (\n                model.contrastive_loss(users, pos_items)\n                if hasattr(model, \"contrastive_loss\")\n                else 0.0\n            )\n            \n            loss = rating_loss + (\n                model.lambda_cl * cl_loss\n                if hasattr(model, \"lambda_cl\")\n                else 0.0\n            )\n\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        model.eval()\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for users, items, ratings in val_loader:\n                users = users.to(device)\n                items = items.to(device)\n                ratings = ratings.to(device)\n\n                preds = model(users, items)\n\n                loss = mse_criterion(preds, ratings)\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n        scheduler.step(val_loss)\n\n        val_rmse = np.sqrt(val_loss) * scaling_factor\n\n        print(\n            f\"Epoch {epoch+1}/{epochs} | \"\n            f\"Train Loss: {train_loss:.4f} | \"\n            f\"Val Loss: {val_loss:.4f} | \"\n            f\"Val RMSE: {val_rmse:.4f}\"\n        )\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), name + \".pth\")\n\n    model.load_state_dict(torch.load(name + \".pth\", map_location=device))\n    print(f\"Best Val RMSE: {np.sqrt(best_val_loss) * scaling_factor:.4f}\")\n    return model\n\n\ndef eval_model(model, test_df, batch_size, device, min_rating, max_rating):\n    test_dataset = RatingDataset(test_df['user_idx'].values, test_df['movie_idx'].values, test_df['rating'].values)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    criterion = nn.MSELoss()\n    model.eval()\n    test_loss = 0\n    with torch.no_grad():\n        for users, movies, ratings in test_loader:\n            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n            predictions = model(users, movies)\n            test_loss += criterion(predictions, ratings).item()\n    test_loss /= len(test_loader)\n\n    scaling_factor = max_rating - min_rating\n    test_rmse_scaled = np.sqrt(test_loss)\n    test_rmse = test_rmse_scaled * scaling_factor\n\n    print(f\"Test RMSE: {test_rmse:.4f}\")\n    return test_rmse\n\ndef make_model(model_type, config, n_users, n_movies, pretrained_models=None):\n    device = config['device']\n    global_mean = data_bundle[0]['rating'].mean()\n\n    if model_type == 'gmf':\n        model = GMF(n_users, n_movies, config['embedding_dim'], global_mean=global_mean)\n        \n    elif model_type == 'lightgbm':\n            if not pretrained_models or 'gmf' not in pretrained_models:\n                 raise ValueError(\"LightGBM requires a pretrained 'gmf' model for embeddings.\")\n                 \n            model = LightGBMRec(\n                n_users=n_users, \n                n_movies=n_movies, \n                gmf_model=pretrained_models['gmf'],\n                train_df=config['train_df'],\n                device=device\n            )\n            return model\n        \n    elif model_type == 'attention':\n        model = AttentionNet(\n            n_users=n_users,\n            n_movies=n_movies,\n            embedding_dim=config['embedding_dim'],\n            hidden_dims=config['hidden_dims'],\n            n_attention_blocks=config['n_attention_blocks'],\n            num_heads=config['n_heads'],\n            dropout=config['dropout'],\n            global_mean=global_mean\n        )\n        \n    elif model_type == 'dmf':\n        model = DMF(\n            n_users=n_users,\n            n_items=n_movies,\n            embedding_dim=config['embedding_dim'],\n            user_hidden_dims=config.get('user_hidden_dims', [128, 64]),\n            item_hidden_dims=config.get('item_hidden_dims', [128, 64]),\n            dropout=config.get('dropout', 0.1),\n            global_mean=global_mean\n        )\n\n    elif model_type == 'ncf':\n        if not pretrained_models or 'gmf' not in pretrained_models or 'attention' not in pretrained_models:\n            raise ValueError(\"NCF requires pretrained 'gmf' and 'attention' models passed in 'pretrained_models'\")\n\n        gmf_pretrained = pretrained_models['gmf']\n        attn_pretrained = pretrained_models['attention']\n\n        model = FusionNCF(gmf_pretrained, attn_pretrained, dropout=config.get('dropout', 0.1))\n\n        if config.get('freeze_pretrained', False):\n            for param in model.gmf.parameters():\n                param.requires_grad = False\n            for param in model.attn_net.parameters():\n                param.requires_grad = False\n                \n    elif model_type == 'nmf':\n        model = NeuMF(\n            n_users=n_users,\n            n_items=n_movies,\n            embedding_dim=config['embedding_dim'],\n            mlp_hidden_dims=config.get('mlp_hidden_dims', [128, 64]),\n            dropout=config.get('dropout', 0.1),\n            global_mean=global_mean\n        )\n\n    elif model_type == 'lightgcn':\n        edge_index = build_edge_index(config['train_df']).to(device)\n    \n        model = LightGCN(\n            n_users=n_users,\n            n_items=n_movies,\n            embedding_dim=config['embedding_dim'],\n            n_layers=config['n_layers'],\n            edge_index=edge_index,\n            global_mean=global_mean\n        )\n        \n    elif model_type == 'lightgcnpp':\n        edge_index = build_edge_index(config['train_df']).to(device)\n    \n        model = LightGCNPP(\n            n_users=n_users,\n            n_items=n_movies,\n            embedding_dim=config['embedding_dim'],\n            n_layers=config['n_layers'],\n            edge_index=edge_index,\n            global_mean=global_mean,\n            residual=True\n        )\n\n    elif model_type == 'simgcl':\n        edge_index = build_edge_index(config['train_df']).to(device)\n    \n        model = SimGCL(\n            n_users=n_users,\n            n_items=n_movies,\n            embedding_dim=config['embedding_dim'],\n            n_layers=config['n_layers'],\n            edge_index=edge_index,\n            global_mean=global_mean,\n            eps=config.get('eps', 0.1),\n            temperature=config.get('temperature', 0.2),\n            lambda_cl=config.get('lambda_cl', 0.1)\n        )\n        \n    elif model_type == 'ensemble':\n        if not pretrained_models:\n            raise ValueError(\"Ensemble requires pretrained_models\")\n    \n        model_list = []\n        for name in config['ensemble_models']:\n            model_list.append(pretrained_models[name])\n    \n        model = Ensemble(\n            models=model_list,\n            learn_weights=config.get('learn_weights', True)\n        )\n\n    else:\n        raise ValueError(f\"Unknown model_type: {model_type}\")\n\n    return model.to(device)\n    \ndef sample(model, test_df, n_samples, device):\n    if n_samples > len(test_df):\n        n_samples = len(test_df)\n\n    sample_df = test_df.sample(n=n_samples).copy()\n\n    model.eval()\n    with torch.no_grad():\n        users = torch.LongTensor(sample_df['user_idx'].values).to(device)\n        movies = torch.LongTensor(sample_df['movie_idx'].values).to(device)\n        predictions = 1 + model(users, movies) * 4\n        predictions = torch.clamp(predictions, 1.0, 5.0).cpu().numpy()\n\n    sample_df['predicted_rating'] = predictions\n    sample_df['rating'] = 1 + 4 * sample_df['rating']\n    sample_df['error'] = sample_df['predicted_rating'] - sample_df['rating']\n    sample_df['absolute_error'] = np.abs(sample_df['error'])\n\n    sample_rmse = np.sqrt(np.mean(sample_df['error'] ** 2))\n    sample_mae = np.mean(sample_df['absolute_error'])\n\n    display_df = sample_df[['userId', 'movieId', 'rating', 'predicted_rating', 'error', 'absolute_error']].head(10)\n    print(display_df.round(2).to_string(index=False))\n\n    print(\"\\n\" + \"-\" * 100)\n    print(f\"RMSE: {sample_rmse:.4f}\")\n    print(f\"MAE: {sample_mae:.4f}\")\n\n    return sample_df, sample_rmse\n\ndef pipeline(model_type, config, data_bundle, pretrained_models=None):\n    train_df, val_df, test_df, _, _, n_users, n_movies, min_rating, max_rating = data_bundle\n\n    model = make_model(model_type, config, n_users, n_movies, pretrained_models)\n\n    model = train(\n        model=model,\n        train_df=train_df,\n        val_df=val_df,\n        batch_size=config['batch_size'],\n        epochs=config['epochs'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        name=f\"best_{model_type}\",\n        device=config['device'],\n        min_rating=min_rating,\n        max_rating=max_rating\n    )\n\n    eval_model(model, test_df, config['batch_size'], config['device'], min_rating, max_rating)\n\n    return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:16:22.920720Z","iopub.execute_input":"2026-01-10T11:16:22.920957Z","iopub.status.idle":"2026-01-10T11:16:23.143892Z","shell.execute_reply.started":"2026-01-10T11:16:22.920941Z","shell.execute_reply":"2026-01-10T11:16:23.143111Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def generate_submission(movie_map, user_map, model, prompt_path, train_df, output_path=\"submission.csv\", device='cuda'):\n    prompt_df = pd.read_csv(prompt_path, sep='\\t', names=['UserId', 'MovieId'])\n    prompt_df['UserIdx'] = prompt_df['UserId'].map(user_map)\n    prompt_df['MovieIdx'] = prompt_df['MovieId'].map(movie_map)\n\n    model.eval()\n    model = model.to(device)\n\n    preds = []\n    with torch.no_grad():\n        for _, row in prompt_df.iterrows():\n            user_idx = row['UserIdx']\n            movie_idx = row['MovieIdx']\n\n            if pd.notna(movie_idx):\n                user_tensor = torch.LongTensor([user_idx]).to(device)\n                movie_tensor = torch.LongTensor([movie_idx]).to(device)\n                output = model(user_tensor, movie_tensor).item()\n                pred = np.clip(1 + output * 4, 1.0, 5.0)\n            else:\n                pred = 4.0\n\n            preds.append(pred)\n\n    submission = pd.DataFrame({\n        \"Id\": np.arange(1, len(preds) + 1),\n        \"Score\": preds\n    })\n\n    submission.to_csv(output_path, index=False)\n    print(f\"Saved submission file to: {output_path}\")\n    print(submission.head())\n\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:52:23.588456Z","iopub.execute_input":"2026-01-10T10:52:23.589156Z","iopub.status.idle":"2026-01-10T10:52:23.594925Z","shell.execute_reply.started":"2026-01-10T10:52:23.589127Z","shell.execute_reply":"2026-01-10T10:52:23.594333Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def check_coverage(movie_map, user_map, prompt_path):\n    prompt_df = pd.read_csv(prompt_path, sep='\\t', names=['UserId', 'MovieId'])\n\n    missing_users = set(prompt_df['UserId']) - set(user_map.keys())\n    missing_movies = set(prompt_df['MovieId']) - set(movie_map.keys())\n\n    ok = True\n    if missing_users:\n        print(f\"Missing {len(missing_users)} users not found in user_map:\")\n        print(list(missing_users)[:10], \"...\" if len(missing_users) > 10 else \"\")\n        ok = False\n\n    if missing_movies:\n        print(f\"Missing {len(missing_movies)} movies not found in movie_map:\")\n        print(list(missing_movies)[:10], \"...\" if len(missing_movies) > 10 else \"\")\n        ok = False\n\n    if ok:\n        print(\"All users and movies are covered.\")\n\n    return ok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:50:20.276834Z","iopub.execute_input":"2026-01-10T10:50:20.277265Z","iopub.status.idle":"2026-01-10T10:50:20.296863Z","shell.execute_reply.started":"2026-01-10T10:50:20.277241Z","shell.execute_reply":"2026-01-10T10:50:20.295976Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDATA_PATH = '/kaggle/input/recsys/train.txt'\n\ndata_bundle = preprocessing(DATA_PATH)\ntrain_df, val_df, test_df, user_map, movie_map, n_users, n_movies, min_value, max_value = data_bundle\nprint(f\"Users: {n_users}, Movies: {n_movies}\")\ntrained_models = {}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:50:20.297726Z","iopub.execute_input":"2026-01-10T10:50:20.297977Z","iopub.status.idle":"2026-01-10T10:50:20.485124Z","shell.execute_reply.started":"2026-01-10T10:50:20.297957Z","shell.execute_reply":"2026-01-10T10:50:20.484409Z"}},"outputs":[{"name":"stdout","text":"Scaling ratings from [1, 5] to [0, 1].\nUsers: 943, Movies: 1680\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"gmf_config = {\n    'epochs': 7,\n    'batch_size': 512,\n    'learning_rate': 0.00065,\n    'weight_decay': 1e-5,\n    'embedding_dim': 64,\n    'device': device\n}\ntrained_models['gmf'] = pipeline('gmf', gmf_config, data_bundle)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:16:33.521186Z","iopub.execute_input":"2026-01-10T11:16:33.521901Z","iopub.status.idle":"2026-01-10T11:16:42.020160Z","shell.execute_reply.started":"2026-01-10T11:16:33.521867Z","shell.execute_reply":"2026-01-10T11:16:42.019501Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_gmf\nEpoch 1/7 | Train Loss: 0.0734 | Val Loss: 0.0669 | Val RMSE: 1.0344\nEpoch 2/7 | Train Loss: 0.0635 | Val Loss: 0.0581 | Val RMSE: 0.9642\nEpoch 3/7 | Train Loss: 0.0549 | Val Loss: 0.0524 | Val RMSE: 0.9152\nEpoch 4/7 | Train Loss: 0.0495 | Val Loss: 0.0498 | Val RMSE: 0.8923\nEpoch 5/7 | Train Loss: 0.0458 | Val Loss: 0.0488 | Val RMSE: 0.8832\nEpoch 6/7 | Train Loss: 0.0422 | Val Loss: 0.0480 | Val RMSE: 0.8766\nEpoch 7/7 | Train Loss: 0.0387 | Val Loss: 0.0479 | Val RMSE: 0.8754\nBest Val RMSE: 0.8754\nTest RMSE: 0.9173\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"dmf_config = {\n    'epochs': 20,\n    'batch_size': 512,\n    'learning_rate': 0.003,\n    'weight_decay': 1e-6,\n    'embedding_dim': 64,\n    'user_hidden_dims': [64, 32],\n    'item_hidden_dims': [64, 32],\n    'dropout': 0.25,\n    'device': device\n}\ntrained_models['dmf'] = pipeline('dmf', dmf_config, data_bundle)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-01-10T10:37:12.978484Z","iopub.execute_input":"2026-01-10T10:37:12.978856Z","iopub.status.idle":"2026-01-10T10:37:36.988987Z","shell.execute_reply.started":"2026-01-10T10:37:12.978829Z","shell.execute_reply":"2026-01-10T10:37:36.988282Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_dmf\nEpoch 1/20 | Train Loss: 0.0712 | Val Loss: 0.0555 | Val RMSE: 0.9427\nEpoch 2/20 | Train Loss: 0.0570 | Val Loss: 0.0533 | Val RMSE: 0.9234\nEpoch 3/20 | Train Loss: 0.0553 | Val Loss: 0.0523 | Val RMSE: 0.9147\nEpoch 4/20 | Train Loss: 0.0547 | Val Loss: 0.0524 | Val RMSE: 0.9157\nEpoch 5/20 | Train Loss: 0.0540 | Val Loss: 0.0533 | Val RMSE: 0.9237\nEpoch 6/20 | Train Loss: 0.0538 | Val Loss: 0.0518 | Val RMSE: 0.9107\nEpoch 7/20 | Train Loss: 0.0536 | Val Loss: 0.0516 | Val RMSE: 0.9085\nEpoch 8/20 | Train Loss: 0.0533 | Val Loss: 0.0519 | Val RMSE: 0.9113\nEpoch 9/20 | Train Loss: 0.0533 | Val Loss: 0.0515 | Val RMSE: 0.9075\nEpoch 10/20 | Train Loss: 0.0531 | Val Loss: 0.0512 | Val RMSE: 0.9050\nEpoch 11/20 | Train Loss: 0.0526 | Val Loss: 0.0511 | Val RMSE: 0.9039\nEpoch 12/20 | Train Loss: 0.0525 | Val Loss: 0.0515 | Val RMSE: 0.9074\nEpoch 13/20 | Train Loss: 0.0524 | Val Loss: 0.0511 | Val RMSE: 0.9045\nEpoch 14/20 | Train Loss: 0.0521 | Val Loss: 0.0527 | Val RMSE: 0.9184\nEpoch 15/20 | Train Loss: 0.0510 | Val Loss: 0.0503 | Val RMSE: 0.8975\nEpoch 16/20 | Train Loss: 0.0509 | Val Loss: 0.0503 | Val RMSE: 0.8968\nEpoch 17/20 | Train Loss: 0.0507 | Val Loss: 0.0511 | Val RMSE: 0.9046\nEpoch 18/20 | Train Loss: 0.0504 | Val Loss: 0.0502 | Val RMSE: 0.8963\nEpoch 19/20 | Train Loss: 0.0503 | Val Loss: 0.0502 | Val RMSE: 0.8959\nEpoch 20/20 | Train Loss: 0.0504 | Val Loss: 0.0504 | Val RMSE: 0.8977\nBest Val RMSE: 0.8959\nTest RMSE: 0.9228\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"attn_config = {\n    'epochs': 11,\n    'batch_size': 512,\n    'learning_rate': 0.00085,\n    'weight_decay': 1e-3,\n    'embedding_dim': 64,\n    'hidden_dims': [128, 64],\n    'n_attention_blocks': 2,\n    'n_heads': 4,\n    'dropout': 0.3,\n    'device': device\n}\ntrained_models['attention'] = pipeline('attention', attn_config, data_bundle)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:16:42.021265Z","iopub.execute_input":"2026-01-10T11:16:42.021462Z","iopub.status.idle":"2026-01-10T11:17:04.967137Z","shell.execute_reply.started":"2026-01-10T11:16:42.021446Z","shell.execute_reply":"2026-01-10T11:17:04.966276Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_attention\nEpoch 1/11 | Train Loss: 0.1083 | Val Loss: 0.0665 | Val RMSE: 1.0312\nEpoch 2/11 | Train Loss: 0.0650 | Val Loss: 0.0588 | Val RMSE: 0.9703\nEpoch 3/11 | Train Loss: 0.0585 | Val Loss: 0.0547 | Val RMSE: 0.9358\nEpoch 4/11 | Train Loss: 0.0560 | Val Loss: 0.0548 | Val RMSE: 0.9364\nEpoch 5/11 | Train Loss: 0.0549 | Val Loss: 0.0531 | Val RMSE: 0.9220\nEpoch 6/11 | Train Loss: 0.0539 | Val Loss: 0.0524 | Val RMSE: 0.9157\nEpoch 7/11 | Train Loss: 0.0536 | Val Loss: 0.0527 | Val RMSE: 0.9187\nEpoch 8/11 | Train Loss: 0.0530 | Val Loss: 0.0532 | Val RMSE: 0.9222\nEpoch 9/11 | Train Loss: 0.0527 | Val Loss: 0.0521 | Val RMSE: 0.9128\nEpoch 10/11 | Train Loss: 0.0523 | Val Loss: 0.0519 | Val RMSE: 0.9112\nEpoch 11/11 | Train Loss: 0.0520 | Val Loss: 0.0512 | Val RMSE: 0.9054\nBest Val RMSE: 0.9054\nTest RMSE: 0.9388\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"ncf_config = {\n    'epochs': 15,\n    'batch_size': 256,\n    'learning_rate': 0.00035,\n    'weight_decay': 0.001,\n    'dropout': 0.3,\n    'freeze_pretrained': False,\n    'device': device\n}\ntrained_models['ncf'] = pipeline('ncf', ncf_config, data_bundle, pretrained_models=trained_models)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:17:04.968222Z","iopub.execute_input":"2026-01-10T11:17:04.968536Z","iopub.status.idle":"2026-01-10T11:18:02.298046Z","shell.execute_reply.started":"2026-01-10T11:17:04.968512Z","shell.execute_reply":"2026-01-10T11:18:02.297225Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_ncf\nEpoch 1/15 | Train Loss: 0.0638 | Val Loss: 0.0493 | Val RMSE: 0.8880\nEpoch 2/15 | Train Loss: 0.0528 | Val Loss: 0.0483 | Val RMSE: 0.8788\nEpoch 3/15 | Train Loss: 0.0515 | Val Loss: 0.0483 | Val RMSE: 0.8789\nEpoch 4/15 | Train Loss: 0.0505 | Val Loss: 0.0486 | Val RMSE: 0.8822\nEpoch 5/15 | Train Loss: 0.0500 | Val Loss: 0.0486 | Val RMSE: 0.8816\nEpoch 6/15 | Train Loss: 0.0480 | Val Loss: 0.0480 | Val RMSE: 0.8766\nEpoch 7/15 | Train Loss: 0.0471 | Val Loss: 0.0484 | Val RMSE: 0.8804\nEpoch 8/15 | Train Loss: 0.0467 | Val Loss: 0.0483 | Val RMSE: 0.8789\nEpoch 9/15 | Train Loss: 0.0463 | Val Loss: 0.0488 | Val RMSE: 0.8841\nEpoch 10/15 | Train Loss: 0.0447 | Val Loss: 0.0498 | Val RMSE: 0.8929\nEpoch 11/15 | Train Loss: 0.0442 | Val Loss: 0.0495 | Val RMSE: 0.8895\nEpoch 12/15 | Train Loss: 0.0439 | Val Loss: 0.0501 | Val RMSE: 0.8955\nEpoch 13/15 | Train Loss: 0.0430 | Val Loss: 0.0501 | Val RMSE: 0.8956\nEpoch 14/15 | Train Loss: 0.0426 | Val Loss: 0.0507 | Val RMSE: 0.9010\nEpoch 15/15 | Train Loss: 0.0424 | Val Loss: 0.0510 | Val RMSE: 0.9033\nBest Val RMSE: 0.8766\nTest RMSE: 0.9540\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"nmf_config = {\n    'epochs': 9,\n    'batch_size': 1024,\n    'learning_rate': 0.0005,\n    'weight_decay': 1e-6,\n    'embedding_dim': 128,\n    'mlp_hidden_dims': [128, 64, 32],\n    'dropout': 0.1,\n    'device': device\n}\ntrained_models['nmf'] = pipeline('nmf', nmf_config, data_bundle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:38:42.365359Z","iopub.execute_input":"2026-01-10T10:38:42.365609Z","iopub.status.idle":"2026-01-10T10:38:51.793642Z","shell.execute_reply.started":"2026-01-10T10:38:42.365584Z","shell.execute_reply":"2026-01-10T10:38:51.793050Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_nmf\nEpoch 1/9 | Train Loss: 0.2065 | Val Loss: 0.0586 | Val RMSE: 0.9681\nEpoch 2/9 | Train Loss: 0.0619 | Val Loss: 0.0550 | Val RMSE: 0.9378\nEpoch 3/9 | Train Loss: 0.0587 | Val Loss: 0.0547 | Val RMSE: 0.9359\nEpoch 4/9 | Train Loss: 0.0569 | Val Loss: 0.0522 | Val RMSE: 0.9143\nEpoch 5/9 | Train Loss: 0.0523 | Val Loss: 0.0502 | Val RMSE: 0.8961\nEpoch 6/9 | Train Loss: 0.0464 | Val Loss: 0.0494 | Val RMSE: 0.8893\nEpoch 7/9 | Train Loss: 0.0406 | Val Loss: 0.0481 | Val RMSE: 0.8775\nEpoch 8/9 | Train Loss: 0.0350 | Val Loss: 0.0489 | Val RMSE: 0.8842\nEpoch 9/9 | Train Loss: 0.0297 | Val Loss: 0.0486 | Val RMSE: 0.8817\nBest Val RMSE: 0.8775\nTest RMSE: 0.9013\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"lightgcn_config = {\n    'epochs': 30,\n    'batch_size': 512,\n    'learning_rate': 0.002,\n    'weight_decay': 1e-6,\n    'embedding_dim': 128,\n    'n_layers': 6,\n    'device': device,\n    'train_df': data_bundle[0]\n}\ntrained_models['lightgcn'] = pipeline('lightgcn', lightgcn_config, data_bundle)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2026-01-10T10:38:51.794405Z","iopub.execute_input":"2026-01-10T10:38:51.794742Z","iopub.status.idle":"2026-01-10T10:41:04.299044Z","shell.execute_reply.started":"2026-01-10T10:38:51.794716Z","shell.execute_reply":"2026-01-10T10:41:04.298354Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_lightgcn\nEpoch 1/30 | Train Loss: 0.1484 | Val Loss: 0.1030 | Val RMSE: 1.2837\nEpoch 2/30 | Train Loss: 0.0895 | Val Loss: 0.0847 | Val RMSE: 1.1641\nEpoch 3/30 | Train Loss: 0.0768 | Val Loss: 0.0751 | Val RMSE: 1.0962\nEpoch 4/30 | Train Loss: 0.0698 | Val Loss: 0.0693 | Val RMSE: 1.0532\nEpoch 5/30 | Train Loss: 0.0656 | Val Loss: 0.0662 | Val RMSE: 1.0289\nEpoch 6/30 | Train Loss: 0.0628 | Val Loss: 0.0633 | Val RMSE: 1.0062\nEpoch 7/30 | Train Loss: 0.0610 | Val Loss: 0.0616 | Val RMSE: 0.9930\nEpoch 8/30 | Train Loss: 0.0597 | Val Loss: 0.0603 | Val RMSE: 0.9822\nEpoch 9/30 | Train Loss: 0.0588 | Val Loss: 0.0592 | Val RMSE: 0.9736\nEpoch 10/30 | Train Loss: 0.0581 | Val Loss: 0.0587 | Val RMSE: 0.9693\nEpoch 11/30 | Train Loss: 0.0575 | Val Loss: 0.0581 | Val RMSE: 0.9642\nEpoch 12/30 | Train Loss: 0.0571 | Val Loss: 0.0575 | Val RMSE: 0.9595\nEpoch 13/30 | Train Loss: 0.0567 | Val Loss: 0.0572 | Val RMSE: 0.9570\nEpoch 14/30 | Train Loss: 0.0563 | Val Loss: 0.0566 | Val RMSE: 0.9513\nEpoch 15/30 | Train Loss: 0.0559 | Val Loss: 0.0561 | Val RMSE: 0.9475\nEpoch 16/30 | Train Loss: 0.0555 | Val Loss: 0.0560 | Val RMSE: 0.9464\nEpoch 17/30 | Train Loss: 0.0550 | Val Loss: 0.0554 | Val RMSE: 0.9415\nEpoch 18/30 | Train Loss: 0.0544 | Val Loss: 0.0549 | Val RMSE: 0.9369\nEpoch 19/30 | Train Loss: 0.0539 | Val Loss: 0.0547 | Val RMSE: 0.9354\nEpoch 20/30 | Train Loss: 0.0533 | Val Loss: 0.0547 | Val RMSE: 0.9358\nEpoch 21/30 | Train Loss: 0.0528 | Val Loss: 0.0541 | Val RMSE: 0.9304\nEpoch 22/30 | Train Loss: 0.0522 | Val Loss: 0.0536 | Val RMSE: 0.9256\nEpoch 23/30 | Train Loss: 0.0518 | Val Loss: 0.0531 | Val RMSE: 0.9213\nEpoch 24/30 | Train Loss: 0.0514 | Val Loss: 0.0528 | Val RMSE: 0.9189\nEpoch 25/30 | Train Loss: 0.0511 | Val Loss: 0.0527 | Val RMSE: 0.9182\nEpoch 26/30 | Train Loss: 0.0508 | Val Loss: 0.0527 | Val RMSE: 0.9181\nEpoch 27/30 | Train Loss: 0.0505 | Val Loss: 0.0526 | Val RMSE: 0.9171\nEpoch 28/30 | Train Loss: 0.0503 | Val Loss: 0.0527 | Val RMSE: 0.9180\nEpoch 29/30 | Train Loss: 0.0500 | Val Loss: 0.0523 | Val RMSE: 0.9152\nEpoch 30/30 | Train Loss: 0.0498 | Val Loss: 0.0522 | Val RMSE: 0.9139\nBest Val RMSE: 0.9139\nTest RMSE: 0.9356\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"lightgcnpp_config = {\n    'epochs': 12,\n    'batch_size': 1024,\n    'learning_rate': 0.001,\n    'weight_decay': 1e-6,\n\n    'embedding_dim': 48,\n    'n_layers': 3,\n\n    'device': device,\n    'train_df': data_bundle[0]\n}\ntrained_models['lightgcnpp'] = pipeline('lightgcnpp', lightgcnpp_config, data_bundle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:05:39.200440Z","iopub.execute_input":"2026-01-10T10:05:39.200916Z","iopub.status.idle":"2026-01-10T10:05:56.383016Z","shell.execute_reply.started":"2026-01-10T10:05:39.200896Z","shell.execute_reply":"2026-01-10T10:05:56.382413Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_lightgcnpp\nEpoch 1/12 | Train Loss: 0.0702 | Val Loss: 0.0628 | Val RMSE: 1.0020\nEpoch 2/12 | Train Loss: 0.0597 | Val Loss: 0.0568 | Val RMSE: 0.9535\nEpoch 3/12 | Train Loss: 0.0536 | Val Loss: 0.0533 | Val RMSE: 0.9235\nEpoch 4/12 | Train Loss: 0.0494 | Val Loss: 0.0512 | Val RMSE: 0.9050\nEpoch 5/12 | Train Loss: 0.0460 | Val Loss: 0.0496 | Val RMSE: 0.8908\nEpoch 6/12 | Train Loss: 0.0429 | Val Loss: 0.0485 | Val RMSE: 0.8813\nEpoch 7/12 | Train Loss: 0.0399 | Val Loss: 0.0482 | Val RMSE: 0.8782\nEpoch 8/12 | Train Loss: 0.0368 | Val Loss: 0.0480 | Val RMSE: 0.8767\nEpoch 9/12 | Train Loss: 0.0338 | Val Loss: 0.0481 | Val RMSE: 0.8774\nEpoch 10/12 | Train Loss: 0.0308 | Val Loss: 0.0486 | Val RMSE: 0.8817\nEpoch 11/12 | Train Loss: 0.0279 | Val Loss: 0.0493 | Val RMSE: 0.8880\nEpoch 12/12 | Train Loss: 0.0250 | Val Loss: 0.0496 | Val RMSE: 0.8910\nBest Val RMSE: 0.8767\nTest RMSE: 0.8973\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"simgcl_config = {\n    'epochs': 30,\n    'batch_size': 1024,\n    'learning_rate': 0.0017,\n    'weight_decay': 1e-6,\n    'embedding_dim': 64,\n    'n_layers': 3,\n\n    'eps': 0.1,\n    'temperature': 0.2,\n    'lambda_cl': 0.1,\n\n    'device': device,\n    'train_df': data_bundle[0]\n}\ntrained_models['simgcl'] = pipeline('simgcl', simgcl_config, data_bundle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:20:13.046469Z","iopub.execute_input":"2026-01-10T10:20:13.047264Z","iopub.status.idle":"2026-01-10T10:21:40.354585Z","shell.execute_reply.started":"2026-01-10T10:20:13.047239Z","shell.execute_reply":"2026-01-10T10:21:40.353951Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_simgcl\nEpoch 1/30 | Train Loss: 0.7550 | Val Loss: 0.0628 | Val RMSE: 1.0023\nEpoch 2/30 | Train Loss: 0.5769 | Val Loss: 0.0581 | Val RMSE: 0.9644\nEpoch 3/30 | Train Loss: 0.5605 | Val Loss: 0.0563 | Val RMSE: 0.9488\nEpoch 4/30 | Train Loss: 0.5525 | Val Loss: 0.0551 | Val RMSE: 0.9394\nEpoch 5/30 | Train Loss: 0.5478 | Val Loss: 0.0546 | Val RMSE: 0.9349\nEpoch 6/30 | Train Loss: 0.5446 | Val Loss: 0.0542 | Val RMSE: 0.9310\nEpoch 7/30 | Train Loss: 0.5424 | Val Loss: 0.0539 | Val RMSE: 0.9290\nEpoch 8/30 | Train Loss: 0.5406 | Val Loss: 0.0537 | Val RMSE: 0.9269\nEpoch 9/30 | Train Loss: 0.5394 | Val Loss: 0.0536 | Val RMSE: 0.9262\nEpoch 10/30 | Train Loss: 0.5383 | Val Loss: 0.0534 | Val RMSE: 0.9244\nEpoch 11/30 | Train Loss: 0.5373 | Val Loss: 0.0535 | Val RMSE: 0.9248\nEpoch 12/30 | Train Loss: 0.5364 | Val Loss: 0.0534 | Val RMSE: 0.9243\nEpoch 13/30 | Train Loss: 0.5358 | Val Loss: 0.0533 | Val RMSE: 0.9234\nEpoch 14/30 | Train Loss: 0.5352 | Val Loss: 0.0534 | Val RMSE: 0.9244\nEpoch 15/30 | Train Loss: 0.5346 | Val Loss: 0.0534 | Val RMSE: 0.9239\nEpoch 16/30 | Train Loss: 0.5341 | Val Loss: 0.0533 | Val RMSE: 0.9235\nEpoch 17/30 | Train Loss: 0.5333 | Val Loss: 0.0533 | Val RMSE: 0.9232\nEpoch 18/30 | Train Loss: 0.5330 | Val Loss: 0.0533 | Val RMSE: 0.9232\nEpoch 19/30 | Train Loss: 0.5329 | Val Loss: 0.0532 | Val RMSE: 0.9229\nEpoch 20/30 | Train Loss: 0.5326 | Val Loss: 0.0532 | Val RMSE: 0.9229\nEpoch 21/30 | Train Loss: 0.5324 | Val Loss: 0.0532 | Val RMSE: 0.9230\nEpoch 22/30 | Train Loss: 0.5322 | Val Loss: 0.0533 | Val RMSE: 0.9232\nEpoch 23/30 | Train Loss: 0.5319 | Val Loss: 0.0533 | Val RMSE: 0.9232\nEpoch 24/30 | Train Loss: 0.5319 | Val Loss: 0.0533 | Val RMSE: 0.9232\nEpoch 25/30 | Train Loss: 0.5318 | Val Loss: 0.0532 | Val RMSE: 0.9230\nEpoch 26/30 | Train Loss: 0.5316 | Val Loss: 0.0532 | Val RMSE: 0.9230\nEpoch 27/30 | Train Loss: 0.5314 | Val Loss: 0.0532 | Val RMSE: 0.9229\nEpoch 28/30 | Train Loss: 0.5314 | Val Loss: 0.0532 | Val RMSE: 0.9230\nEpoch 29/30 | Train Loss: 0.5312 | Val Loss: 0.0532 | Val RMSE: 0.9230\nEpoch 30/30 | Train Loss: 0.5312 | Val Loss: 0.0532 | Val RMSE: 0.9230\nBest Val RMSE: 0.9229\nTest RMSE: 0.9399\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"ensemble_models = ['lightgcn', 'ncf', 'gmf', 'simgcl']\nfor model_name in ensemble_models:\n    state = torch.load(f\"best_{model_name}.pth\", map_location=\"cpu\")\n    trained_models[model_name].load_state_dict(state)\n\nensemble_config = {\n    'epochs': 8,\n    'batch_size': 1024,\n    'learning_rate': 0.04,\n    'weight_decay': 0.0001,\n    'ensemble_models': ensemble_models,\n    'learn_weights': True,\n    'device': device\n}\ntrained_models['ensemble'] = pipeline('ensemble', ensemble_config, data_bundle, trained_models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T10:41:04.300419Z","iopub.execute_input":"2026-01-10T10:41:04.300834Z","iopub.status.idle":"2026-01-10T10:41:21.875234Z","shell.execute_reply.started":"2026-01-10T10:41:04.300817Z","shell.execute_reply":"2026-01-10T10:41:21.874571Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model: best_ensemble\nEpoch 1/8 | Train Loss: 0.0468 | Val Loss: 0.0503 | Val RMSE: 0.8969\nEpoch 2/8 | Train Loss: 0.0467 | Val Loss: 0.0503 | Val RMSE: 0.8968\nEpoch 3/8 | Train Loss: 0.0467 | Val Loss: 0.0502 | Val RMSE: 0.8961\nEpoch 4/8 | Train Loss: 0.0467 | Val Loss: 0.0502 | Val RMSE: 0.8959\nEpoch 5/8 | Train Loss: 0.0467 | Val Loss: 0.0502 | Val RMSE: 0.8964\nEpoch 6/8 | Train Loss: 0.0467 | Val Loss: 0.0502 | Val RMSE: 0.8962\nEpoch 7/8 | Train Loss: 0.0467 | Val Loss: 0.0502 | Val RMSE: 0.8963\nEpoch 8/8 | Train Loss: 0.0467 | Val Loss: 0.0501 | Val RMSE: 0.8956\nBest Val RMSE: 0.8956\nTest RMSE: 0.9129\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"check_coverage(movie_map, user_map, \"/kaggle/input/recsys/test.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = 'ncf'\nstate = torch.load(f\"best_{model_name}.pth\", map_location=\"cpu\")\ntrained_models[model_name].load_state_dict(state)\ntrained_models[model_name].eval()\nsample(\n    model=trained_models[model_name],\n    test_df=test_df,\n    n_samples=700,\n    device=device\n)\nNone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:18:06.216742Z","iopub.execute_input":"2026-01-10T11:18:06.217538Z","iopub.status.idle":"2026-01-10T11:18:06.243098Z","shell.execute_reply.started":"2026-01-10T11:18:06.217506Z","shell.execute_reply":"2026-01-10T11:18:06.242243Z"}},"outputs":[{"name":"stdout","text":" userId  movieId  rating  predicted_rating  error  absolute_error\n    250      271     4.0              3.36  -0.64            0.64\n    222      145     2.0              1.81  -0.19            0.19\n    889       81     4.0              3.45  -0.55            0.55\n    208      523     4.0              3.80  -0.20            0.20\n    312      614     4.0              4.08   0.08            0.08\n    293      419     3.0              3.22   0.22            0.22\n    422      672     3.0              2.74  -0.26            0.26\n    220      289     4.0              3.31  -0.69            0.69\n    151      736     4.0              4.22   0.22            0.22\n    884      640     1.0              3.44   2.44            2.44\n\n----------------------------------------------------------------------------------------------------\nRMSE: 0.9383\nMAE: 0.7351\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"submission = generate_submission(\n    movie_map=movie_map,\n    user_map=user_map,\n    train_df=train_df,\n    model=trained_models[model_name],\n    prompt_path=\"/kaggle/input/recsys/test.txt\",\n    output_path=f\"{model_name}.csv\",\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:18:07.791863Z","iopub.execute_input":"2026-01-10T11:18:07.792358Z","iopub.status.idle":"2026-01-10T11:18:25.784065Z","shell.execute_reply.started":"2026-01-10T11:18:07.792336Z","shell.execute_reply":"2026-01-10T11:18:25.783306Z"}},"outputs":[{"name":"stdout","text":"Saved submission file to: ncf.csv\n   Id     Score\n0   1  3.882270\n1   2  3.730960\n2   3  4.067957\n3   4  3.261101\n4   5  2.164522\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}