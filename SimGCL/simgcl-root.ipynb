{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":24064,"databundleVersionId":1675974,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.sparse import coo_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:54:11.796252Z","iopub.execute_input":"2026-01-09T00:54:11.796465Z","iopub.status.idle":"2026-01-09T00:54:17.651033Z","shell.execute_reply.started":"2026-01-09T00:54:11.796439Z","shell.execute_reply":"2026-01-09T00:54:17.650323Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load data\ntrain_data = pd.read_csv(\n    '/kaggle/input/movie-recomendation-fall-2020/train.txt',\n    sep='\\t',\n    header=None,\n    names=['user_id', 'movie_id', 'rating']\n)\n\n# Implicit feedback\ntrain_data = train_data[train_data['rating'] > 0]\n\n# Re-index\nuser_map = {u: i for i, u in enumerate(train_data['user_id'].unique())}\nitem_map = {i: j for j, i in enumerate(train_data['movie_id'].unique())}\n\ntrain_data['uid'] = train_data['user_id'].map(user_map)\ntrain_data['iid'] = train_data['movie_id'].map(item_map)\n\nnum_users = len(user_map)\nnum_items = len(item_map)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:54:17.652621Z","iopub.execute_input":"2026-01-09T00:54:17.652942Z","iopub.status.idle":"2026-01-09T00:54:17.724094Z","shell.execute_reply.started":"2026-01-09T00:54:17.652926Z","shell.execute_reply":"2026-01-09T00:54:17.723452Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_adj_matrix(df, num_users, num_items):\n    row = np.concatenate([df['uid'], df['iid'] + num_users])\n    col = np.concatenate([df['iid'] + num_users, df['uid']])\n    data = np.ones(len(row))\n\n    adj = coo_matrix(\n        (data, (row, col)),\n        shape=(num_users + num_items, num_users + num_items)\n    )\n\n    # D^{-1/2} A D^{-1/2}\n    deg = np.array(adj.sum(axis=1)).flatten()\n    deg_inv_sqrt = np.power(deg, -0.5)\n    deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n\n    D_inv_sqrt = coo_matrix(\n        (deg_inv_sqrt, (np.arange(len(deg)), np.arange(len(deg))))\n    )\n\n    norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n    return norm_adj\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:54:17.724882Z","iopub.execute_input":"2026-01-09T00:54:17.725148Z","iopub.status.idle":"2026-01-09T00:54:17.731322Z","shell.execute_reply.started":"2026-01-09T00:54:17.725120Z","shell.execute_reply":"2026-01-09T00:54:17.730466Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"norm_adj = build_adj_matrix(train_data, num_users, num_items)\n\nnorm_adj = norm_adj.tocoo()  # ðŸ”¥ Báº®T BUá»˜C\n\nnorm_adj = torch.sparse_coo_tensor(\n    torch.LongTensor([norm_adj.row, norm_adj.col]),\n    torch.FloatTensor(norm_adj.data),\n    torch.Size(norm_adj.shape)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:45.499606Z","iopub.execute_input":"2026-01-09T00:56:45.500485Z","iopub.status.idle":"2026-01-09T00:56:45.625608Z","shell.execute_reply.started":"2026-01-09T00:56:45.500449Z","shell.execute_reply":"2026-01-09T00:56:45.624350Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/1564793057.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  torch.LongTensor([norm_adj.row, norm_adj.col]),\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class BPRDataset(Dataset):\n    def __init__(self, df, num_items):\n        self.users = df['uid'].values\n        self.items = df['iid'].values\n        self.num_items = num_items\n        self.user_pos = df.groupby('uid')['iid'].apply(set).to_dict()\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        u = self.users[idx]\n        i = self.items[idx]\n\n        while True:\n            j = np.random.randint(self.num_items)\n            if j not in self.user_pos[u]:\n                break\n        return u, i, j\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:48.124549Z","iopub.execute_input":"2026-01-09T00:56:48.125153Z","iopub.status.idle":"2026-01-09T00:56:48.130403Z","shell.execute_reply.started":"2026-01-09T00:56:48.125130Z","shell.execute_reply":"2026-01-09T00:56:48.129609Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SimGCL(nn.Module):\n    def __init__(\n        self,\n        num_users,\n        num_items,\n        emb_dim=64,\n        n_layers=3,\n        noise_eps=0.1,\n        tau=0.2\n    ):\n        super().__init__()\n        self.num_users = num_users\n        self.num_items = num_items\n        self.n_layers = n_layers\n        self.noise_eps = noise_eps\n        self.tau = tau\n\n        self.embedding = nn.Embedding(num_users + num_items, emb_dim)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n    def lightgcn(self, adj):\n        all_emb = self.embedding.weight\n        embs = [all_emb]\n\n        for _ in range(self.n_layers):\n            all_emb = torch.sparse.mm(adj, all_emb)\n            embs.append(all_emb)\n\n        return torch.mean(torch.stack(embs), dim=0)\n\n    def forward(self, adj, add_noise=False):\n        emb = self.lightgcn(adj)\n\n        if add_noise:\n            noise = F.normalize(torch.rand_like(emb), dim=1)\n            emb = emb + self.noise_eps * noise\n\n        user_emb = emb[:self.num_users]\n        item_emb = emb[self.num_users:]\n        return user_emb, item_emb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:50.218148Z","iopub.execute_input":"2026-01-09T00:56:50.218835Z","iopub.status.idle":"2026-01-09T00:56:50.225713Z","shell.execute_reply.started":"2026-01-09T00:56:50.218796Z","shell.execute_reply":"2026-01-09T00:56:50.224901Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def bpr_loss(u_emb, i_emb, j_emb):\n    pos = torch.sum(u_emb * i_emb, dim=1)\n    neg = torch.sum(u_emb * j_emb, dim=1)\n    return -torch.mean(F.logsigmoid(pos - neg))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:53.412106Z","iopub.execute_input":"2026-01-09T00:56:53.412408Z","iopub.status.idle":"2026-01-09T00:56:53.417101Z","shell.execute_reply.started":"2026-01-09T00:56:53.412386Z","shell.execute_reply":"2026-01-09T00:56:53.416080Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def simgcl_cl_loss(z1, z2, tau):\n    z1 = F.normalize(z1, dim=1)\n    z2 = F.normalize(z2, dim=1)\n\n    sim = torch.matmul(z1, z2.T) / tau\n    labels = torch.arange(sim.size(0), device=sim.device)\n    return F.cross_entropy(sim, labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:58.562196Z","iopub.execute_input":"2026-01-09T00:56:58.563099Z","iopub.status.idle":"2026-01-09T00:56:58.567740Z","shell.execute_reply.started":"2026-01-09T00:56:58.563064Z","shell.execute_reply":"2026-01-09T00:56:58.567028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = SimGCL(num_users, num_items).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ndataset = BPRDataset(train_data, num_items)\nloader = DataLoader(dataset, batch_size=2048, shuffle=True)\n\nnorm_adj = norm_adj.to(device)\n\nlambda_cl = 0.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:56:59.964195Z","iopub.execute_input":"2026-01-09T00:56:59.964768Z","iopub.status.idle":"2026-01-09T00:57:03.033500Z","shell.execute_reply.started":"2026-01-09T00:56:59.964743Z","shell.execute_reply":"2026-01-09T00:57:03.032642Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"for epoch in range(1, 101):\n    model.train()\n    total_loss = 0\n\n    for u, i, j in loader:\n        u, i, j = u.to(device), i.to(device), j.to(device)\n\n        # View 1\n        u1, it1 = model(norm_adj, add_noise=True)\n        # View 2\n        u2, it2 = model(norm_adj, add_noise=True)\n\n        # BPR\n        loss_bpr = bpr_loss(\n            u1[u], it1[i], it1[j]\n        )\n\n        # CL\n        loss_cl = (\n            simgcl_cl_loss(u1[u], u2[u], model.tau) +\n            simgcl_cl_loss(it1[i], it2[i], model.tau)\n        )\n\n        loss = loss_bpr + lambda_cl * loss_cl\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch:03d} | Loss {total_loss / len(loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T00:57:03.034843Z","iopub.execute_input":"2026-01-09T00:57:03.035310Z","iopub.status.idle":"2026-01-09T00:59:20.767811Z","shell.execute_reply.started":"2026-01-09T00:57:03.035283Z","shell.execute_reply":"2026-01-09T00:59:20.767023Z"}},"outputs":[{"name":"stdout","text":"Epoch 001 | Loss 2.5814\nEpoch 002 | Loss 2.2277\nEpoch 003 | Loss 2.1263\nEpoch 004 | Loss 2.0755\nEpoch 005 | Loss 2.0439\nEpoch 006 | Loss 2.0221\nEpoch 007 | Loss 2.0061\nEpoch 008 | Loss 1.9937\nEpoch 009 | Loss 1.9839\nEpoch 010 | Loss 1.9755\nEpoch 011 | Loss 1.9691\nEpoch 012 | Loss 1.9631\nEpoch 013 | Loss 1.9578\nEpoch 014 | Loss 1.9533\nEpoch 015 | Loss 1.9494\nEpoch 016 | Loss 1.9460\nEpoch 017 | Loss 1.9428\nEpoch 018 | Loss 1.9399\nEpoch 019 | Loss 1.9373\nEpoch 020 | Loss 1.9348\nEpoch 021 | Loss 1.9323\nEpoch 022 | Loss 1.9303\nEpoch 023 | Loss 1.9281\nEpoch 024 | Loss 1.9263\nEpoch 025 | Loss 1.9248\nEpoch 026 | Loss 1.9230\nEpoch 027 | Loss 1.9214\nEpoch 028 | Loss 1.9197\nEpoch 029 | Loss 1.9183\nEpoch 030 | Loss 1.9170\nEpoch 031 | Loss 1.9154\nEpoch 032 | Loss 1.9144\nEpoch 033 | Loss 1.9130\nEpoch 034 | Loss 1.9120\nEpoch 035 | Loss 1.9105\nEpoch 036 | Loss 1.9092\nEpoch 037 | Loss 1.9083\nEpoch 038 | Loss 1.9070\nEpoch 039 | Loss 1.9058\nEpoch 040 | Loss 1.9049\nEpoch 041 | Loss 1.9037\nEpoch 042 | Loss 1.9024\nEpoch 043 | Loss 1.9016\nEpoch 044 | Loss 1.9005\nEpoch 045 | Loss 1.8993\nEpoch 046 | Loss 1.8982\nEpoch 047 | Loss 1.8973\nEpoch 048 | Loss 1.8963\nEpoch 049 | Loss 1.8949\nEpoch 050 | Loss 1.8940\nEpoch 051 | Loss 1.8928\nEpoch 052 | Loss 1.8914\nEpoch 053 | Loss 1.8905\nEpoch 054 | Loss 1.8890\nEpoch 055 | Loss 1.8880\nEpoch 056 | Loss 1.8871\nEpoch 057 | Loss 1.8858\nEpoch 058 | Loss 1.8843\nEpoch 059 | Loss 1.8831\nEpoch 060 | Loss 1.8818\nEpoch 061 | Loss 1.8807\nEpoch 062 | Loss 1.8790\nEpoch 063 | Loss 1.8776\nEpoch 064 | Loss 1.8760\nEpoch 065 | Loss 1.8748\nEpoch 066 | Loss 1.8735\nEpoch 067 | Loss 1.8718\nEpoch 068 | Loss 1.8706\nEpoch 069 | Loss 1.8683\nEpoch 070 | Loss 1.8667\nEpoch 071 | Loss 1.8650\nEpoch 072 | Loss 1.8632\nEpoch 073 | Loss 1.8613\nEpoch 074 | Loss 1.8593\nEpoch 075 | Loss 1.8575\nEpoch 076 | Loss 1.8552\nEpoch 077 | Loss 1.8533\nEpoch 078 | Loss 1.8510\nEpoch 079 | Loss 1.8487\nEpoch 080 | Loss 1.8462\nEpoch 081 | Loss 1.8436\nEpoch 082 | Loss 1.8411\nEpoch 083 | Loss 1.8387\nEpoch 084 | Loss 1.8357\nEpoch 085 | Loss 1.8327\nEpoch 086 | Loss 1.8301\nEpoch 087 | Loss 1.8266\nEpoch 088 | Loss 1.8235\nEpoch 089 | Loss 1.8208\nEpoch 090 | Loss 1.8167\nEpoch 091 | Loss 1.8137\nEpoch 092 | Loss 1.8099\nEpoch 093 | Loss 1.8064\nEpoch 094 | Loss 1.8026\nEpoch 095 | Loss 1.7984\nEpoch 096 | Loss 1.7945\nEpoch 097 | Loss 1.7903\nEpoch 098 | Loss 1.7867\nEpoch 099 | Loss 1.7818\nEpoch 100 | Loss 1.7775\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\n\n# 1. Load test\ntest_df = pd.read_csv(\n    '/kaggle/input/movie-recomendation-fall-2020/test.txt',\n    sep='\\t',\n    header=None,\n    names=['user_id', 'movie_id']\n)\n\n# 2. Táº¡o Id\ntest_df['Id'] = np.arange(1, len(test_df) + 1)\n\n# 3. Map index\ntest_df['uid'] = test_df['user_id'].map(user_map)\ntest_df['iid'] = test_df['movie_id'].map(item_map)\n\n# 4. Model eval\nmodel.eval()\nwith torch.no_grad():\n    user_emb, item_emb = model(norm_adj, add_noise=False)\n\n# 5. Vectorized prediction\nuids = torch.LongTensor(test_df['uid'].fillna(-1).values).to(device)\niids = torch.LongTensor(test_df['iid'].fillna(-1).values).to(device)\n\nscores = torch.zeros(len(test_df), device=device)\n\nvalid_mask = (uids >= 0) & (iids >= 0)\n\nscores[valid_mask] = torch.sum(\n    user_emb[uids[valid_mask]] * item_emb[iids[valid_mask]],\n    dim=1\n)\n\n# User/item má»›i â†’ score = 0 (implicit CF chuáº©n)\nscores[~valid_mask] = 0.0\n\n# 6. Táº¡o submission\nsubmit_df = pd.DataFrame({\n    'Id': test_df['Id'],\n    'Score': scores.cpu().numpy()\n})\n\nsubmit_path = '/kaggle/working/SimGCL_root.csv'\nsubmit_df.to_csv(submit_path, index=False)\n\nprint(\"âœ… Submission saved:\", submit_path)\nprint(submit_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T01:08:29.861360Z","iopub.execute_input":"2026-01-09T01:08:29.861762Z","iopub.status.idle":"2026-01-09T01:08:30.019097Z","shell.execute_reply.started":"2026-01-09T01:08:29.861741Z","shell.execute_reply":"2026-01-09T01:08:30.018398Z"}},"outputs":[{"name":"stdout","text":"âœ… Submission saved: /kaggle/working/SimGCL_root.csv\n   Id     Score\n0   1 -0.032680\n1   2 -0.125063\n2   3  0.050505\n3   4  0.119273\n4   5 -0.035640\n","output_type":"stream"}],"execution_count":27}]}