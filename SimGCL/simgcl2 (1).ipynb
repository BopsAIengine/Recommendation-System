{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":24064,"databundleVersionId":1675974,"sourceType":"competition"},{"sourceId":1187,"sourceType":"datasetVersion","datasetId":626,"isSourceIdPinned":false}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load data\nprint(\"Loading data...\")\ntrain_data = pd.read_csv('/kaggle/input/movie-recomendation-fall-2020/train.txt', sep='\\t', header=None, names=['user_id', 'movie_id', 'rating'])\n\nprint(f\"Dataset shape: {train_data.shape}\")\nprint(f\"Number of unique users: {train_data['user_id'].nunique()}\")\nprint(f\"Number of unique movies: {train_data['movie_id'].nunique()}\")\nprint(f\"Rating range: {train_data['rating'].min()} - {train_data['rating'].max()}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:08:36.269501Z","iopub.execute_input":"2026-01-09T17:08:36.269692Z","iopub.status.idle":"2026-01-09T17:08:42.670326Z","shell.execute_reply.started":"2026-01-09T17:08:36.269671Z","shell.execute_reply":"2026-01-09T17:08:42.669646Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data...\nDataset shape: (90570, 3)\nNumber of unique users: 943\nNumber of unique movies: 1680\nRating range: 1 - 5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_ids = sorted(train_data['user_id'].unique())  # unique(): L·∫•y ra danh s√°ch c√°c ph·∫ßn t·ª≠ duy nh·∫•t trong c·ªôt; sorted():S·∫Øp x·∫øp theo th·ª© t·ª± t·ª´ l·ªõn ->b√©\nuser_id_mapping = {id: i for i, id in enumerate(user_ids)} # Dictionary Chuy·ªÉn id th√†nh s·ªë theo th·ª© t·ª± \nitem_ids = sorted(train_data['movie_id'].unique())  # 1 to 1682\nitem_id_mapping = {id: i for i, id in enumerate(item_ids)}\n\ntrain_data['user_id'] = train_data['user_id'].map(user_id_mapping) # L√∫c n√†y t·∫•t c·∫£ c√°c id ƒë·ªÅu theo th·ª© t·ª± v√† kh√¥ng b·ªã m·∫•t d·ªØ li·ªáu th·ª© t·ª± c·ªßa user_id\ntrain_data['movie_id'] = train_data['movie_id'].map(item_id_mapping)\n\nnum_users = len(user_ids)\nnum_items = len(item_ids)\n\nprint(\"Number of unique users:\", num_users)\nprint(\"Number of unique items:\", num_items)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:08:42.671777Z","iopub.execute_input":"2026-01-09T17:08:42.672400Z","iopub.status.idle":"2026-01-09T17:08:42.691120Z","shell.execute_reply.started":"2026-01-09T17:08:42.672372Z","shell.execute_reply":"2026-01-09T17:08:42.690346Z"}},"outputs":[{"name":"stdout","text":"Number of unique users: 943\nNumber of unique items: 1680\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_interactions = torch.tensor(train_data[['user_id', 'movie_id']].values, dtype=torch.long)\nprint(train_interactions.shape)\nprint(train_interactions[:10])\nprint(\"Data type:\", train_interactions.dtype)\nprint(\"Device:\", train_interactions.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:08:42.691923Z","iopub.execute_input":"2026-01-09T17:08:42.692140Z","iopub.status.idle":"2026-01-09T17:08:42.758627Z","shell.execute_reply.started":"2026-01-09T17:08:42.692118Z","shell.execute_reply":"2026-01-09T17:08:42.758077Z"}},"outputs":[{"name":"stdout","text":"torch.Size([90570, 2])\ntensor([[0, 0],\n        [0, 1],\n        [0, 2],\n        [0, 3],\n        [0, 4],\n        [0, 5],\n        [0, 6],\n        [0, 7],\n        [0, 8],\n        [0, 9]])\nData type: torch.int64\nDevice: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Adjacency matrix\nrows = torch.cat([train_interactions[:, 0], train_interactions[:, 1] + num_users], dim=0) #N·ªëi 2 c·ªôt user_id v√† movie_id ƒë·ªÉ h·ªçc t∆∞∆°ng t√°c\ncols = torch.cat([train_interactions[:, 1] + num_users, train_interactions[:, 0]], dim=0)\n# rows = [0, 0, 1, 4, 5, 3]  ## C√°c ph·∫ßn t·ª≠ trong rows v√† cols l√† gi·ªëng nhau, ch·ªâ kh√°c nhau v·ªÅ th·ª© t·ª± ƒë·ªÉ h·ªçc c√°c t∆∞∆°ng t√°c\n# cols = [4, 5, 3, 0, 0, 1]\n#          ‚Üì   ‚Üì  ‚Üì  ‚Üì  ‚Üì  ‚Üì\n# C·∫°nh: (0,4), (0,5), (1,3), (4,0), (5,0), (3,1)\nindices = torch.stack([rows, cols], dim=0).to(device)\nvalues = torch.ones(indices.shape[1], device=device) #values :Tensor to√†n s·ªë 1 v√† indices.shape[1] th·ªÉ hi·ªán cho s·ªë c·∫°nh \nadj = torch.sparse_coo_tensor(indices, values, size=(num_users + num_items, num_users + num_items), device=device)\n#adj :Ma tr·∫≠n k·ªÅ bi·ªÉu di·ªÖn t∆∞∆°ng t√°c gi·ªØa item v√† user\n# Normalized adjacency matrix\ndegrees = torch.sparse.sum(adj, dim=1).to_dense() #T√≠nh b·∫≠c c·ªßa c√°c node b·∫±ng c√°c t√≠nh t·ªïng t·ª´ng h√†ng->degrees: m·∫£ng bi·ªÉu di·ªÖn b·∫≠c c·ªßa t·ª´ng node\nnorm_values = 1.0 / (torch.sqrt(degrees[rows]) * torch.sqrt(degrees[cols])).to(device) #T√≠nh h·ªá s·ªë c·∫°nh gi·ªØa user v√† item\n#A_norm[u, i] = 1 / (‚àödegree(u) * ‚àödegree(i)):Ph·∫ßn t·ª≠ ƒë·∫°i di·ªán cho g√≠a tr·ªã c·∫°nh trong ma tr·∫≠n norm_adj\nnorm_adj = torch.sparse_coo_tensor(indices, norm_values, size=(num_users + num_items, num_users + num_items), device=device) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:08:42.759577Z","iopub.execute_input":"2026-01-09T17:08:42.759901Z","iopub.status.idle":"2026-01-09T17:08:43.200788Z","shell.execute_reply.started":"2026-01-09T17:08:42.759869Z","shell.execute_reply":"2026-01-09T17:08:43.200032Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SimGCL(nn.Module):\n    def __init__(self, num_users, num_items, embedding_dim, num_layers, norm_adj, device):\n        super(SimGCL, self).__init__()\n        self.num_users = num_users\n        self.num_items = num_items\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n        self.device = device\n        self.register_buffer('norm_adj', norm_adj)\n        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n        nn.init.normal_(self.user_embeddings.weight, std=0.01)  # Initialize user embeddings\n        nn.init.normal_(self.item_embeddings.weight, std=0.01)  # Initialize item embeddings\n        self.eps = 0.1 \n\n    #H√†m n√†y truy·ªÅn th√¥ng tin qua c√°c  layer GNN ƒë·ªÉ t·∫°o ra c√°c Embedding cu·ªëi c√πng cho user v√† item\n    def forward(self, perturbed=False):\n        # Concatenate initial user and item embeddings\n        ego_embeddings = torch.cat([self.user_embeddings.weight, self.item_embeddings.weight], dim=0)\n        #self.user_embedding: (num_user,embedding_dim); self.item_embedding: (num_item,embedding_dim)\n        ## ---> ego_embeddings: (num_user+num_item,embedding_dim):Gh√©p 2 ma tr·∫≠n tr√™n l·∫°i vs nhau\n        #ego_embeddings ch·ªâ th·ªÉ hi·ªán embedding c·ªßa c√°c node m√† ko c√≥ s·ª± t∆∞∆°ng t√°c  gi·ªØa c√°c node \n        all_embeddings = []\n\n        #L·∫∑p qua s·ªë l∆∞·ª£ng c√°c layer GNN\n        for k in range(self.num_layers):\n            \n            ego_embeddings = torch.spmm(self.norm_adj, ego_embeddings) # Cho norm_adj(num_item+num_user;num_item+num_user)*(num_item+num_user;embeddings_dim)\n           #ego_embeddings: l√∫c n√†y l√† c√°c embedding th·ªÉ hi·ªán ƒëc ƒë·∫∑c tr∆∞ng c·ªßa node(ƒë√£ bao g·ªìm vs s·ª± t∆∞∆°ng t√°c c·ªßa c√°c node kh√°c vs n√≥)\n\n            #Nhi·ªÖu n√†y kh√¥ng ch·ªâ th√™m 1 l·∫ßn v√†o ban ƒë·∫ßu m√† th√™m v√†o m·ªói l·ªõp GNN\n            #Th√™m nhi·ªÖu v√†o m√¥ h√¨nh \n            if perturbed:\n                #Ma tr·∫≠n nhi·ªÖm t·ª´ [0-1]\n                random_noise = torch.rand_like(ego_embeddings).to(self.device)\n                # Add normalized noise scaled by eps\n                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(random_noise, dim=-1) * self.eps\n\n            #Qua m·ªói l·ªõp GNN, l∆∞u l·∫°i to√†n b·ªô embedding ƒë·∫°i di·ªán cho c√°c node\n            all_embeddings.append(ego_embeddings)\n\n        # Stack embeddings across layers and compute the mean\n        all_embeddings = torch.stack(all_embeddings, dim=1)\n        all_embeddings = torch.mean(all_embeddings, dim=1)\n\n        # Split into user and item embeddings\n        user_all_embeddings, item_all_embeddings = torch.split(\n            all_embeddings, [self.num_users, self.num_items]) \n # chia ma tr·∫≠n all_embeddings(num_user+item_user,embedding) th√†nh 2 ma tr·∫≠n user_all_embedding(num_user,embedding)v√† item_all_embedding(item_user,embedding)\n\n        return user_all_embeddings, item_all_embeddings\n\n    def get_embeddings(self):\n        all_embeddings = torch.cat([self.user_embeddings.weight, self.item_embeddings.weight], dim=0)\n        ego_embeddings = all_embeddings\n        for _ in range(self.num_layers):\n            all_embeddings = torch.spmm(self.norm_adj, all_embeddings)\n            ego_embeddings += all_embeddings\n        final_embeddings = ego_embeddings / (self.num_layers + 1)\n        user_emb = final_embeddings[:self.num_users]\n        item_emb = final_embeddings[self.num_users:]\n        return user_emb, item_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:08:43.201757Z","iopub.execute_input":"2026-01-09T17:08:43.202064Z","iopub.status.idle":"2026-01-09T17:08:43.211234Z","shell.execute_reply.started":"2026-01-09T17:08:43.202042Z","shell.execute_reply":"2026-01-09T17:08:43.210590Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch.optim as optim\n# Hyperparameters\nembedding_dim = 64\nnum_layers = 3\nlearning_rate =1e-3\nlambda_reg = 1e-6  # Regularization weight\ncl_rate = 0.01  # Contrastive loss weight\n\n# Initialize model\nmodel = SimGCL(num_users, num_items, embedding_dim, num_layers, norm_adj, device)\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:10:43.334626Z","iopub.execute_input":"2026-01-09T17:10:43.335002Z","iopub.status.idle":"2026-01-09T17:10:46.163585Z","shell.execute_reply.started":"2026-01-09T17:10:43.334974Z","shell.execute_reply":"2026-01-09T17:10:46.162793Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def InfoNCE(view1, view2, temperature: float, b_cos: bool = True):\n    if b_cos:\n        view1, view2 = F.normalize(view1, dim=1), F.normalize(view2, dim=1)\n    pos_score = (view1 @ view2.T) / temperature\n    score = torch.diag(F.log_softmax(pos_score, dim=1))\n    return -score.mean()\n\ndef bpr_loss(user_emb, pos_item_emb, neg_item_emb):\n    pos_score = (user_emb * pos_item_emb).sum(dim=1)\n    neg_score = (user_emb * neg_item_emb).sum(dim=1)\n    loss = -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8)\n    return loss.mean()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:10:46.164784Z","iopub.execute_input":"2026-01-09T17:10:46.165241Z","iopub.status.idle":"2026-01-09T17:10:46.170261Z","shell.execute_reply.started":"2026-01-09T17:10:46.165218Z","shell.execute_reply":"2026-01-09T17:10:46.169632Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_simgcl(\n    model,\n    train_interactions,\n    num_epochs=50,\n    batch_size=256,\n    temperature=0.2,\n    learning_rate=0.001,\n    lambda_cl=0.1\n):\n    device = model.device\n    model.train()\n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    train_dataset = TensorDataset(\n        train_interactions[:, 0],  # user\n        train_interactions[:, 1]   # pos item\n    )\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    for epoch in range(num_epochs):\n        total_loss = 0\n\n        for batch_users, batch_pos_items in train_loader:\n            batch_users = batch_users.to(device)\n            batch_pos_items = batch_pos_items.to(device)\n\n            # --------- Negative sampling ---------\n            batch_neg_items = torch.randint(\n                low=0,\n                high=model.num_items,\n                size=batch_pos_items.size(),\n                device=device\n            )\n\n            optimizer.zero_grad()\n\n            # ==================================================\n            # 1. BPR LOSS (NO perturbation)\n            # ==================================================\n            user_emb, item_emb = model(perturbed=False)\n\n            u_emb = user_emb[batch_users]\n            pos_emb = item_emb[batch_pos_items]\n            neg_emb = item_emb[batch_neg_items]\n\n            loss_bpr = bpr_loss(u_emb, pos_emb, neg_emb)\n\n            # ==================================================\n            # 2. CONTRASTIVE LOSS (WITH perturbation)\n            # ==================================================\n            user_emb1, item_emb1 = model(perturbed=True)\n            user_emb2, item_emb2 = model(perturbed=True)\n\n            cl_user = InfoNCE(\n                user_emb1[batch_users],\n                user_emb2[batch_users],\n                temperature\n            )\n\n            cl_item = InfoNCE(\n                item_emb1[batch_pos_items],\n                item_emb2[batch_pos_items],\n                temperature\n            )\n\n            loss_cl = cl_user + cl_item\n\n            # ==================================================\n            # 3. JOINT LOSS\n            # ==================================================\n            loss = loss_bpr + lambda_cl * loss_cl\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}] | \"\n            f\"Loss: {total_loss / len(train_loader):.4f} | \"\n            f\"BPR: {loss_bpr.item():.4f} | CL: {loss_cl.item():.4f}\"\n        )\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:23:09.617942Z","iopub.execute_input":"2026-01-09T17:23:09.618294Z","iopub.status.idle":"2026-01-09T17:23:09.626592Z","shell.execute_reply.started":"2026-01-09T17:23:09.618268Z","shell.execute_reply":"2026-01-09T17:23:09.625889Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":" train_interactions_tensor = torch.tensor(train_interactions, dtype=torch.long).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:23:12.474572Z","iopub.execute_input":"2026-01-09T17:23:12.475192Z","iopub.status.idle":"2026-01-09T17:23:12.479747Z","shell.execute_reply.started":"2026-01-09T17:23:12.475163Z","shell.execute_reply":"2026-01-09T17:23:12.479136Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/2303990061.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_interactions_tensor = torch.tensor(train_interactions, dtype=torch.long).to(device)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\ntrained_model = train_simgcl(\n        model=model,\n        train_interactions=train_interactions_tensor,\n        num_epochs=50,\n        batch_size=256,\n        temperature=0.2,\n        learning_rate=0.001\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:23:14.156375Z","iopub.execute_input":"2026-01-09T17:23:14.156669Z","iopub.status.idle":"2026-01-09T17:31:15.467016Z","shell.execute_reply.started":"2026-01-09T17:23:14.156644Z","shell.execute_reply":"2026-01-09T17:31:15.466314Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/50] | Loss: 0.9398 | BPR: 0.6865 | CL: 2.1589\nEpoch [2/50] | Loss: 0.9385 | BPR: 0.6856 | CL: 2.1898\nEpoch [3/50] | Loss: 0.9376 | BPR: 0.6842 | CL: 2.2012\nEpoch [4/50] | Loss: 0.9366 | BPR: 0.6814 | CL: 2.1593\nEpoch [5/50] | Loss: 0.9357 | BPR: 0.6824 | CL: 2.1644\nEpoch [6/50] | Loss: 0.9342 | BPR: 0.6802 | CL: 2.2465\nEpoch [7/50] | Loss: 0.9327 | BPR: 0.6794 | CL: 2.2315\nEpoch [8/50] | Loss: 0.9312 | BPR: 0.6802 | CL: 2.2227\nEpoch [9/50] | Loss: 0.9299 | BPR: 0.6694 | CL: 2.2504\nEpoch [10/50] | Loss: 0.9280 | BPR: 0.6673 | CL: 2.2031\nEpoch [11/50] | Loss: 0.9261 | BPR: 0.6735 | CL: 2.1967\nEpoch [12/50] | Loss: 0.9242 | BPR: 0.6688 | CL: 2.2405\nEpoch [13/50] | Loss: 0.9218 | BPR: 0.6696 | CL: 2.2431\nEpoch [14/50] | Loss: 0.9196 | BPR: 0.6493 | CL: 2.2510\nEpoch [15/50] | Loss: 0.9170 | BPR: 0.6351 | CL: 2.3024\nEpoch [16/50] | Loss: 0.9145 | BPR: 0.6655 | CL: 2.2218\nEpoch [17/50] | Loss: 0.9116 | BPR: 0.6545 | CL: 2.2363\nEpoch [18/50] | Loss: 0.9082 | BPR: 0.6685 | CL: 2.2793\nEpoch [19/50] | Loss: 0.9059 | BPR: 0.6431 | CL: 2.2914\nEpoch [20/50] | Loss: 0.9025 | BPR: 0.6421 | CL: 2.2667\nEpoch [21/50] | Loss: 0.8990 | BPR: 0.6240 | CL: 2.2681\nEpoch [22/50] | Loss: 0.8958 | BPR: 0.6334 | CL: 2.2997\nEpoch [23/50] | Loss: 0.8915 | BPR: 0.6395 | CL: 2.2687\nEpoch [24/50] | Loss: 0.8882 | BPR: 0.6139 | CL: 2.3068\nEpoch [25/50] | Loss: 0.8848 | BPR: 0.6218 | CL: 2.2890\nEpoch [26/50] | Loss: 0.8802 | BPR: 0.6247 | CL: 2.3724\nEpoch [27/50] | Loss: 0.8761 | BPR: 0.5979 | CL: 2.3664\nEpoch [28/50] | Loss: 0.8712 | BPR: 0.5973 | CL: 2.3745\nEpoch [29/50] | Loss: 0.8664 | BPR: 0.5938 | CL: 2.3819\nEpoch [30/50] | Loss: 0.8613 | BPR: 0.5782 | CL: 2.3927\nEpoch [31/50] | Loss: 0.8562 | BPR: 0.5612 | CL: 2.4865\nEpoch [32/50] | Loss: 0.8501 | BPR: 0.5620 | CL: 2.4702\nEpoch [33/50] | Loss: 0.8430 | BPR: 0.5400 | CL: 2.5230\nEpoch [34/50] | Loss: 0.8353 | BPR: 0.5416 | CL: 2.6426\nEpoch [35/50] | Loss: 0.8269 | BPR: 0.5037 | CL: 2.6490\nEpoch [36/50] | Loss: 0.8183 | BPR: 0.5276 | CL: 2.7451\nEpoch [37/50] | Loss: 0.8083 | BPR: 0.4843 | CL: 2.7946\nEpoch [38/50] | Loss: 0.7974 | BPR: 0.4673 | CL: 2.8985\nEpoch [39/50] | Loss: 0.7858 | BPR: 0.4173 | CL: 2.9639\nEpoch [40/50] | Loss: 0.7734 | BPR: 0.4491 | CL: 3.1776\nEpoch [41/50] | Loss: 0.7625 | BPR: 0.4185 | CL: 3.1323\nEpoch [42/50] | Loss: 0.7515 | BPR: 0.4128 | CL: 3.1435\nEpoch [43/50] | Loss: 0.7403 | BPR: 0.3708 | CL: 3.1240\nEpoch [44/50] | Loss: 0.7318 | BPR: 0.3881 | CL: 3.2379\nEpoch [45/50] | Loss: 0.7213 | BPR: 0.3495 | CL: 3.2230\nEpoch [46/50] | Loss: 0.7116 | BPR: 0.3122 | CL: 3.2312\nEpoch [47/50] | Loss: 0.7050 | BPR: 0.3559 | CL: 3.2062\nEpoch [48/50] | Loss: 0.6968 | BPR: 0.3314 | CL: 3.1484\nEpoch [49/50] | Loss: 0.6907 | BPR: 0.3650 | CL: 3.1851\nEpoch [50/50] | Loss: 0.6831 | BPR: 0.3209 | CL: 3.2018\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\n# 1. T·∫£i l·∫°i t·∫≠p test v·ªõi ƒë√∫ng ƒë·ªãnh d·∫°ng\ntest_df = pd.read_csv('/kaggle/input/movie-recomendation-fall-2020/test.txt', \n                     sep='\\t', \n                     header=None,\n                     names=['user_id', 'movie_id'])\n\nprint(\"Test data shape:\", test_df.shape)\nprint(\"Test data columns:\", test_df.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(test_df.head())\nprint(\"\\nD·ªØ li·ªáu test:\")\nprint(test_df.info())\n\n# 2. T·∫°o c·ªôt Id cho file submit (t·ª´ 1 ƒë·∫øn s·ªë l∆∞·ª£ng m·∫´u)\ntest_df['Id'] = range(1, len(test_df) + 1)\n\n# 3. √Ånh x·∫° user_id v√† movie_id sang index ƒë√£ chu·∫©n h√≥a\ntest_df['user_id_mapped'] = test_df['user_id'].map(user_id_mapping)\ntest_df['movie_id_mapped'] = test_df['movie_id'].map(item_id_mapping)\n\n# ƒê·∫øm s·ªë user/item kh√¥ng c√≥ trong t·∫≠p train\nmissing_users = test_df['user_id_mapped'].isna().sum()\nmissing_items = test_df['movie_id_mapped'].isna().sum()\nprint(f\"\\nS·ªë user kh√¥ng c√≥ trong t·∫≠p train: {missing_users}/{len(test_df)}\")\nprint(f\"S·ªë item kh√¥ng c√≥ trong t·∫≠p train: {missing_items}/{len(test_df)}\")\n\n# 4. ƒê·∫£m b·∫£o model ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√°\ntrained_model.eval()\n\n# 5. L·∫•y embeddings t·ª´ model\nwith torch.no_grad():\n    user_emb, item_emb = trained_model.get_embeddings()\n\n# 6. T·∫°o d·ª± ƒëo√°n\npredictions = []\n\nfor _, row in test_df.iterrows():\n    user_idx = row['user_id_mapped']\n    item_idx = row['movie_id_mapped']\n    \n    # Ki·ªÉm tra xem user/item c√≥ trong t·∫≠p train kh√¥ng\n    if pd.isna(user_idx) or pd.isna(item_idx):\n        # N·∫øu user ho·∫∑c item m·ªõi, d√πng rating trung b√¨nh = 2.5\n        pred_rating = 2.5\n    else:\n        # Chuy·ªÉn sang integer\n        user_idx = int(user_idx)\n        item_idx = int(item_idx)\n        \n        # L·∫•y embeddings\n        user_embedding = user_emb[user_idx]\n        item_embedding = item_emb[item_idx]\n        \n        # T√≠nh rating d·ª± ƒëo√°n b·∫±ng dot product\n        pred_rating = torch.dot(user_embedding, item_embedding).item()\n        \n        # Chu·∫©n h√≥a rating v·ªÅ kho·∫£ng [1, 5]\n        # C√°ch 1: D√πng sigmoid function\n        pred_rating = 1 + 4 * (1 / (1 + np.exp(-pred_rating)))\n        \n        # C√°ch 2: Ho·∫∑c scale d·ª±a tr√™n min-max (c·∫ßn t√≠nh t·ª´ train)\n        # Gi·∫£ s·ª≠ dot product trong kho·∫£ng [-10, 10]\n        # pred_rating = (pred_rating + 10) / 20 * 4 + 1\n        \n        # ƒê·∫£m b·∫£o trong kho·∫£ng [1, 5]\n        pred_rating = max(1.0, min(5.0, pred_rating))\n    \n    predictions.append(pred_rating)\n\n# 7. T·∫°o file submit\ntest_df['Score'] = predictions\n\n# Ch·ªâ l·∫•y 2 c·ªôt c·∫ßn thi·∫øt cho submission\nsubmit_df = test_df[['Id', 'Score']].copy()\n\n# 8. L∆∞u file submit\nsubmit_path = '/kaggle/working/SimGCL2.csv'\nsubmit_df.to_csv(submit_path, index=False)\n\nprint(f\"\\n‚úÖ File submit ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {submit_path}\")\nprint(f\"K√≠ch th∆∞·ªõc file: {submit_df.shape}\")\nprint(f\"\\n10 d√≤ng ƒë·∫ßu ti√™n c·ªßa file submit:\")\nprint(submit_df.head(10))\nprint(f\"\\nTh·ªëng k√™ rating d·ª± ƒëo√°n:\")\nprint(submit_df['Score'].describe())\n\n# 9. Ki·ªÉm tra ƒë·ªãnh d·∫°ng file\nprint(f\"\\nüìã Ki·ªÉm tra file submit:\")\nsample = pd.read_csv(submit_path)\nprint(f\"C·ªôt: {sample.columns.tolist()}\")\nprint(f\"Ki·ªÉu d·ªØ li·ªáu: {sample.dtypes.tolist()}\")\nprint(f\"Min Score: {sample['Score'].min():.4f}\")\nprint(f\"Max Score: {sample['Score'].max():.4f}\")\nprint(f\"Mean Score: {sample['Score'].mean():.4f}\")\n\n# 10. L∆∞u th√™m b·∫£n backup ƒë·ªÉ ki·ªÉm tra\nbackup_path = '/kaggle/working/submission_with_details.csv'\ntest_df[['Id', 'user_id', 'movie_id', 'user_id_mapped', 'movie_id_mapped', 'Score']].to_csv(backup_path, index=False)\nprint(f\"\\nüìÅ File backup chi ti·∫øt: {backup_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T17:31:45.635404Z","iopub.execute_input":"2026-01-09T17:31:45.635731Z","iopub.status.idle":"2026-01-09T17:31:46.811518Z","shell.execute_reply.started":"2026-01-09T17:31:45.635706Z","shell.execute_reply":"2026-01-09T17:31:46.810898Z"}},"outputs":[{"name":"stdout","text":"Test data shape: (9430, 2)\nTest data columns: ['user_id', 'movie_id']\n\nFirst few rows:\n   user_id  movie_id\n0        1        20\n1        1        33\n2        1        61\n3        1       117\n4        1       155\n\nD·ªØ li·ªáu test:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9430 entries, 0 to 9429\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype\n---  ------    --------------  -----\n 0   user_id   9430 non-null   int64\n 1   movie_id  9430 non-null   int64\ndtypes: int64(2)\nmemory usage: 147.5 KB\nNone\n\nS·ªë user kh√¥ng c√≥ trong t·∫≠p train: 0/9430\nS·ªë item kh√¥ng c√≥ trong t·∫≠p train: 2/9430\n\n‚úÖ File submit ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: /kaggle/working/SimGCL2.csv\nK√≠ch th∆∞·ªõc file: (9430, 2)\n\n10 d√≤ng ƒë·∫ßu ti√™n c·ªßa file submit:\n   Id     Score\n0   1  4.999327\n1   2  1.110645\n2   3  3.881274\n3   4  3.088794\n4   5  4.762532\n5   6  3.679695\n6   7  4.930430\n7   8  4.987152\n8   9  1.886794\n9  10  1.696910\n\nTh·ªëng k√™ rating d·ª± ƒëo√°n:\ncount    9430.000000\nmean        4.097367\nstd         1.298602\nmin         1.000001\n25%         3.567720\n50%         4.856670\n75%         4.992523\nmax         5.000000\nName: Score, dtype: float64\n\nüìã Ki·ªÉm tra file submit:\nC·ªôt: ['Id', 'Score']\nKi·ªÉu d·ªØ li·ªáu: [dtype('int64'), dtype('float64')]\nMin Score: 1.0000\nMax Score: 5.0000\nMean Score: 4.0974\n\nüìÅ File backup chi ti·∫øt: /kaggle/working/submission_with_details.csv\n","output_type":"stream"}],"execution_count":16}]}